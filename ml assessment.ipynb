{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ff9db0",
   "metadata": {},
   "source": [
    "Q-1. Imagine you have a dataset where you have different Instagram features\n",
    "like u sername , Caption , Hashtag , Followers , Time_Since_posted , and likes , now your task is\n",
    "to predict the number of likes and Time Since posted and the rest of the features are\n",
    "your input features. Now you have to build a model which can predict the\n",
    "number of likes and Time Since posted.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa31e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - Mean Squared Error: 569.1148100667051\n",
      "Likes - Mean Absolute Error: 18.883724960718947\n",
      "Time Since Posted - Mean Squared Error: 0.00018247565722000601\n",
      "Time Since Posted - Mean Absolute Error: 0.008308204910866212\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"instagram_reach.csv\")\n",
    "\n",
    "# Separate the categorical and numerical columns\n",
    "categorical_cols = ['USERNAME', 'Caption', 'Hashtags']\n",
    "numerical_cols = ['Followers']\n",
    "\n",
    "# Perform one-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Preprocess 'Time since posted' column\n",
    "data['Time_Since_posted'] = data['Time since posted'].apply(lambda x: re.findall(r'\\d+', x)[0]).astype(int)\n",
    "\n",
    "# Concatenate the encoded features with the numerical columns\n",
    "X = pd.concat([encoded_df, data[numerical_cols], data['Time_Since_posted']], axis=1)\n",
    "\n",
    "# Split the dataset into input features (X) and target variables (y)\n",
    "y_likes = data['Likes']\n",
    "y_time_since_posted = data['Time_Since_posted']\n",
    "\n",
    "# Split the data into training and testing sets for likes\n",
    "X_train_likes, X_test_likes, y_train_likes, y_test_likes = train_test_split(X, y_likes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model for predicting likes\n",
    "model_likes = LinearRegression()\n",
    "model_likes.fit(X_train_likes, y_train_likes)\n",
    "\n",
    "# Make predictions on the test set for likes\n",
    "y_pred_likes = model_likes.predict(X_test_likes)\n",
    "\n",
    "# Evaluate the model for likes\n",
    "mse_likes = mean_squared_error(y_test_likes, y_pred_likes)\n",
    "mae_likes = mean_absolute_error(y_test_likes, y_pred_likes)\n",
    "print(\"Likes - Mean Squared Error:\", mse_likes)\n",
    "print(\"Likes - Mean Absolute Error:\", mae_likes)\n",
    "\n",
    "# Split the data into training and testing sets for time since posted\n",
    "X_train_time, X_test_time, y_train_time, y_test_time = train_test_split(X, y_time_since_posted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model for predicting time since posted\n",
    "model_time = LinearRegression()\n",
    "model_time.fit(X_train_time, y_train_time)\n",
    "\n",
    "# Make predictions on the test set for time since posted\n",
    "y_pred_time = model_time.predict(X_test_time)\n",
    "\n",
    "# Evaluate the model for time since posted\n",
    "mse_time = mean_squared_error(y_test_time, y_pred_time)\n",
    "mae_time = mean_absolute_error(y_test_time, y_pred_time)\n",
    "print(\"Time Since Posted - Mean Squared Error:\", mse_time)\n",
    "print(\"Time Since Posted - Mean Absolute Error:\", mae_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f84e94",
   "metadata": {},
   "source": [
    "Q-2. Imagine you have a dataset where you have different features like Age ,\n",
    "Gender , Height , Weight , BMI , and Blood Pressure and you have to classify the people into\n",
    "different classes like Normal , Overweight , Obesity , Underweight , and Extreme Obesity by using\n",
    "any 4 different classification algorithms. Now you have to build a model which\n",
    "can classify people into different classes.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7358c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6784869976359338\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.82      0.89      0.85        56\n",
      "      Normal_Weight       0.67      0.53      0.59        62\n",
      "     Obesity_Type_I       0.59      0.53      0.55        78\n",
      "    Obesity_Type_II       0.78      0.88      0.83        58\n",
      "   Obesity_Type_III       0.81      1.00      0.89        63\n",
      " Overweight_Level_I       0.58      0.50      0.54        56\n",
      "Overweight_Level_II       0.40      0.42      0.41        50\n",
      "\n",
      "           accuracy                           0.68       423\n",
      "          macro avg       0.67      0.68      0.67       423\n",
      "       weighted avg       0.67      0.68      0.67       423\n",
      "\n",
      "Decision Tree Accuracy: 0.9361702127659575\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.90      0.96      0.93        56\n",
      "      Normal_Weight       0.89      0.87      0.88        62\n",
      "     Obesity_Type_I       0.97      0.94      0.95        78\n",
      "    Obesity_Type_II       0.95      0.95      0.95        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.91      0.89      0.90        56\n",
      "Overweight_Level_II       0.92      0.94      0.93        50\n",
      "\n",
      "           accuracy                           0.94       423\n",
      "          macro avg       0.93      0.94      0.93       423\n",
      "       weighted avg       0.94      0.94      0.94       423\n",
      "\n",
      "Random Forest Accuracy: 0.9456264775413712\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.95      0.96      0.96        56\n",
      "      Normal_Weight       0.85      0.92      0.88        62\n",
      "     Obesity_Type_I       0.99      0.94      0.96        78\n",
      "    Obesity_Type_II       0.97      0.98      0.97        58\n",
      "   Obesity_Type_III       1.00      1.00      1.00        63\n",
      " Overweight_Level_I       0.92      0.88      0.90        56\n",
      "Overweight_Level_II       0.94      0.94      0.94        50\n",
      "\n",
      "           accuracy                           0.95       423\n",
      "          macro avg       0.95      0.95      0.94       423\n",
      "       weighted avg       0.95      0.95      0.95       423\n",
      "\n",
      "Support Vector Machines Accuracy: 0.5650118203309693\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.71      0.88      0.78        56\n",
      "      Normal_Weight       0.48      0.34      0.40        62\n",
      "     Obesity_Type_I       0.62      0.33      0.43        78\n",
      "    Obesity_Type_II       0.77      0.40      0.52        58\n",
      "   Obesity_Type_III       0.56      1.00      0.72        63\n",
      " Overweight_Level_I       0.47      0.50      0.49        56\n",
      "Overweight_Level_II       0.43      0.58      0.50        50\n",
      "\n",
      "           accuracy                           0.57       423\n",
      "          macro avg       0.58      0.57      0.55       423\n",
      "       weighted avg       0.58      0.57      0.54       423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "# Split the dataset into input features (X) and target variable (y)\n",
    "X = data[['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC',\n",
    "          'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']]\n",
    "y = data['NObeyesdad']\n",
    "\n",
    "# Perform one-hot encoding for categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred_dtree = dtree.predict(X_test)\n",
    "accuracy_dtree = accuracy_score(y_test, y_pred_dtree)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dtree)\n",
    "print(classification_report(y_test, y_pred_dtree))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Support Vector Machines\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Support Vector Machines Accuracy:\", accuracy_svm)\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f065bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff31041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores:\n",
      "[0.         0.         0.         ... 0.         0.         0.01179082]\n",
      "Euclidean Distances:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_json(\"News_Category_Dataset_v3.json\", lines=True)\n",
    "\n",
    "# Select the features or categories of data\n",
    "selected_data = data[['headline', 'category', 'short_description']]\n",
    "\n",
    "# Convert the selected data into a list of dictionaries\n",
    "data_list = selected_data.to_dict('records')\n",
    "\n",
    "# Given data\n",
    "given_data = {\n",
    "    \"link\": \"https://www.huffpost.com/entry/virginia-thomas-agrees-to-interview-with-jan-6-panel_n_632ba0f2e4b09d8701bbe16d\",\n",
    "    \"headline\": \"Virginia Thomas Agrees To Interview With Jan. 6 Panel\",\n",
    "    \"category\": \"U.S. NEWS\",\n",
    "    \"short_description\": \"Conservative activist Virginia Thomas, the wife of Supreme Court Justice Clarence Thomas, has agreed to participate in a voluntary interview with the House panel investigating the Jan. 6 insurrection.\",\n",
    "    \"authors\": \"Eric Tucker and Mary Clare Jalonick, AP\",\n",
    "    \"date\": \"2022-09-21\"\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the given data\n",
    "given_df = pd.DataFrame([given_data])\n",
    "\n",
    "# Concatenate the given data with the dataset\n",
    "combined_data = pd.concat([selected_data, given_df], ignore_index=True)\n",
    "\n",
    "# Apply TF-IDF vectorization to the combined data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(combined_data['headline'])\n",
    "\n",
    "# Get the vector representation of the given data\n",
    "given_vector = vectors[-1]\n",
    "\n",
    "# Calculate the similarity scores using different algorithms\n",
    "cosine_sim_scores = cosine_similarity(given_vector, vectors[:-1]).flatten()\n",
    "euclidean_distances = [distance.euclidean(given_vector.toarray().flatten(), vec.toarray().flatten()) for vec in vectors[:-1]]\n",
    "pearson_correlation = [pearsonr(given_vector.toarray().flatten(), vec.toarray().flatten())[0] for vec in vectors[:-1]]\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"Cosine Similarity Scores:\")\n",
    "print(cosine_sim_scores)\n",
    "print(\"Euclidean Distances:\")\n",
    "print(euclidean_distances)\n",
    "print(\"Pearson Correlation Coefficients:\")\n",
    "print(pearson_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bf1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.889294403892944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "data = pd.read_csv('online_shoppers_intention.csv')  # Replace 'your_dataset.csv' with the actual dataset filename\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()  # Remove rows with missing values, or use imputation techniques\n",
    "\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data, columns=['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType'])\n",
    "\n",
    "# Step 2: Split the data\n",
    "X = data.drop(['Revenue'], axis=1)\n",
    "y = data['Revenue']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Choose an ensemble learning algorithm\n",
    "ensemble_model = RandomForestClassifier()\n",
    "\n",
    "# Step 5: Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Make predictions\n",
    "# Replace the values with the actual input values to make predictions\n",
    "input_data = pd.DataFrame({\n",
    "    'Administrative': [5],\n",
    "    'Administrative_Duration': [100],\n",
    "    'Informational': [2],\n",
    "    'Informational_Duration': [50],\n",
    "    'ProductRelated': [10],\n",
    "    'ProductRelated_Duration': [200],\n",
    "    'BounceRates': [0.2],\n",
    "    'ExitRates': [0.4],\n",
    "    'PageValues': [15],\n",
    "    'SpecialDay': [0],\n",
    "    'Month_Jan': [0],\n",
    "    'Month_Feb': [1],\n",
    "    'OperatingSystems_1': [0],\n",
    "    'OperatingSystems_2': [1],\n",
    "    'Browser_1': [0],\n",
    "    'Browser_2': [1],\n",
    "    'Region_1': [0],\n",
    "    'Region_2': [1],\n",
    "    'TrafficType_1': [0],\n",
    "    'TrafficType_2': [1],\n",
    "    'VisitorType_New_Visitor': [0],\n",
    "    'VisitorType_Returning_Visitor': [1],\n",
    "    'Weekend': [1]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcbc9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Month_Jan\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Browser_10\n",
      "- Browser_11\n",
      "- Browser_12\n",
      "- Browser_13\n",
      "- Browser_3\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 23 features, but RandomForestClassifier is expecting 74 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict revenue\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m revenue_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevenue prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, revenue_prediction)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict weekend or not\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:850\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    848\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 23 features, but RandomForestClassifier is expecting 74 features as input."
     ]
    }
   ],
   "source": [
    "# Predict revenue\n",
    "revenue_prediction = ensemble_model.predict(input_data)\n",
    "print(\"Revenue prediction:\", revenue_prediction)\n",
    "\n",
    "# Predict weekend or not\n",
    "weekend_prediction = \"Weekend\" if input_data['Weekend'][0] == 1 else \"Not Weekend\"\n",
    "print(\"Weekend prediction:\", weekend_prediction)\n",
    "\n",
    "# Predict informational duration\n",
    "input_data_without_info_duration = input_data.drop('Informational_Duration', axis=1)\n",
    "info_duration_prediction = ensemble_model.predict(input_data_without_info_duration)\n",
    "print(\"Informational Duration prediction:\", info_duration_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dedbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import folium\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Unsupervised Learning - Clustering for High Booking Areas\n",
    "X_cluster = data[['latitude', 'longitude']]  # Input features for clustering\n",
    "\n",
    "# Determine the optimal number of clusters using elbow method\n",
    "inertia = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Select the number of clusters based on the elbow plot\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 3  # Example: Select 3 clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_cluster)\n",
    "\n",
    "# Assign cluster labels to the data points\n",
    "data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# Supervised Learning - Regression for Price Prediction\n",
    "X_regression = data[['latitude', 'longitude', 'cluster_label']]  # Input features for regression\n",
    "y_price = data['price']  # Target variable for price prediction\n",
    "\n",
    "# Train the linear regression model\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_regression, y_price)\n",
    "\n",
    "# Map Visualization\n",
    "# Create a map centered on a specific location\n",
    "m = folium.Map(location=[latitude_center, longitude_center], zoom_start=12)\n",
    "\n",
    "# Add markers for each data point with cluster labels\n",
    "for index, row in data.iterrows():\n",
    "    folium.Marker([row['latitude'], row['longitude']],\n",
    "                  popup=f\"Cluster: {row['cluster_label']}\",\n",
    "                  icon=folium.Icon(color='blue')).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m.save('map.html')\n",
    "\n",
    "# Predict the price for a new location\n",
    "new_location = [[new_latitude, new_longitude, cluster_label]]  # Example: New location information\n",
    "predicted_price = regression_model.predict(new_location)\n",
    "print(\"Predicted Price:\", predicted_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa62e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8978102189781022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Month_Jan\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Browser_10\n",
      "- Browser_11\n",
      "- Browser_12\n",
      "- Browser_13\n",
      "- Browser_3\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 23 features, but RandomForestClassifier is expecting 74 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdministrative\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdministrative_Duration\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekend\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     60\u001b[0m })\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Predict revenue\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m revenue_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevenue prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, revenue_prediction)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:850\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    848\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 23 features, but RandomForestClassifier is expecting 74 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "data = pd.read_csv('online_shoppers_intention.csv')  # Replace 'your_dataset.csv' with the actual dataset filename\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()  # Remove rows with missing values, or use imputation techniques\n",
    "\n",
    "# Encode categorical variables\n",
    "data = pd.get_dummies(data, columns=['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType'])\n",
    "\n",
    "# Step 2: Split the data\n",
    "X = data.drop(['Revenue'], axis=1)\n",
    "y = data['Revenue']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Choose an ensemble learning algorithm\n",
    "num_features_input = X_train.shape[1]\n",
    "ensemble_model = RandomForestClassifier(n_estimators=100, max_features=num_features_input)\n",
    "\n",
    "# Step 5: Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Make predictions\n",
    "# Replace the values with the actual input values to make predictions\n",
    "input_data = pd.DataFrame({\n",
    "    'Administrative': [5],\n",
    "    'Administrative_Duration': [100],\n",
    "    'Informational': [2],\n",
    "    'Informational_Duration': [50],\n",
    "    'ProductRelated': [10],\n",
    "    'ProductRelated_Duration': [200],\n",
    "    'BounceRates': [0.2],\n",
    "    'ExitRates': [0.4],\n",
    "    'PageValues': [15],\n",
    "    'SpecialDay': [0],\n",
    "    'Month_Jan': [0],\n",
    "    'Month_Feb': [1],\n",
    "    'OperatingSystems_1': [0],\n",
    "    'OperatingSystems_2': [1],\n",
    "    'Browser_1': [0],\n",
    "    'Browser_2': [1],\n",
    "    'Region_1': [0],\n",
    "    'Region_2': [1],\n",
    "    'TrafficType_1': [0],\n",
    "    'TrafficType_2': [1],\n",
    "    'VisitorType_New_Visitor': [0],\n",
    "    'VisitorType_Returning_Visitor': [1],\n",
    "    'Weekend': [1]\n",
    "})\n",
    "\n",
    "# Predict revenue\n",
    "revenue_prediction = ensemble_model.predict(input_data)\n",
    "print(\"Revenue prediction:\", revenue_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80136935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\abhis\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abhis\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from flask) (8.0.4)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from flask) (2.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from flask) (2.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from click>=5.1->flask) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ff686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [06/Jun/2023 19:36:31] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return 'Welcome to the Music Genres Prediction API'\n",
    "\n",
    "@app.route('/favicon.ico', methods=['GET'])\n",
    "def favicon():\n",
    "    return send_from_directory(os.path.join(app.root_path, 'static'), 'favicon.ico', mimetype='image/vnd.microsoft.icon')\n",
    "\n",
    "def predict_genres():\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('data_2genre.csv')\n",
    "\n",
    "    # Extract features for clustering\n",
    "    features = data.drop(['filename', 'label'], axis=1)\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply clustering algorithm\n",
    "    kmeans = MiniBatchKMeans(n_clusters=5, batch_size=100)  # Adjust the number of clusters and batch size as per your requirement\n",
    "    predicted_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "    # Add predicted labels to the dataset\n",
    "    data['predicted_label'] = predicted_labels\n",
    "\n",
    "    # Convert the result to JSON\n",
    "    result = data[['filename', 'predicted_label']].to_dict(orient='records')\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return 'Welcome to the Music Genres Prediction API'\n",
    "\n",
    "@app.route('/favicon.ico', methods=['GET'])\n",
    "def favicon():\n",
    "    return send_from_directory(os.path.join(app.root_path, 'static'), 'favicon.ico', mimetype='image/vnd.microsoft.icon')\n",
    "\n",
    "def predict_genres():\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv('data_2genre.csv')\n",
    "\n",
    "        # Extract features for clustering\n",
    "        features = data.drop(['filename', 'label'], axis=1)\n",
    "\n",
    "        # Normalize the features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "        # Apply clustering algorithm\n",
    "        kmeans = MiniBatchKMeans(n_clusters=5, batch_size=100)  # Adjust the number of clusters and batch size as per your requirement\n",
    "        predicted_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "        # Add predicted labels to the dataset\n",
    "        data['predicted_label'] = predicted_labels\n",
    "\n",
    "        # Convert the result to JSON\n",
    "        result = data[['filename', 'predicted_label']].to_dict(orient='records')\n",
    "\n",
    "        return jsonify(result)\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that occur during the prediction process\n",
    "        error_message = f\"Error occurred during prediction: {str(e)}\"\n",
    "        return jsonify({'error': error_message})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079414ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "html_code = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <title>Music Genres Prediction</title>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Music Genres Prediction</h1>\n",
    "\n",
    "  <div id=\"predictions\"></div>\n",
    "\n",
    "  <script>\n",
    "    fetch('http://localhost:5000/predict')\n",
    "      .then(response => response.json())\n",
    "      .then(data => {\n",
    "        const predictionsElement = document.getElementById('predictions');\n",
    "        data.forEach(item => {\n",
    "          const predictionItem = document.createElement('p');\n",
    "          predictionItem.textContent = `${item.filename}: ${item.predicted_label}`;\n",
    "          predictionsElement.appendChild(predictionItem);\n",
    "        });\n",
    "      })\n",
    "      .catch(error => console.error('Error:', error));\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "display(HTML(html_code))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753ba61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "9.\n",
    "pip install scikit-learn pandas numpy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)  # Features\n",
    "y = df['target']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict malware using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('accepted', axis=1)  # Features\n",
    "y = df['accepted']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "rest steps are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59395689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4faa09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
